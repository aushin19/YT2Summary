this  van  model  is  giving  me  an  orgasmic 
 Rush  look  at  these  videos  and  you  Marvel 
 where  the  AI  has  come  and  this  model  is 
 either  just  1.3  billion  or  14  billion 
 and  look  at  the  physical  simulation 
 cinematic  quality  and  you  just  wonder 
 where  what  sort  of  time  we  are  living  in 
 look  at  this  quality  look  at  this  one 
 look  at  all  the  details  and  nitt  which 
 this  model  is  producing  just  on  the 
 basis  of  text  or  images  I  already  have 
 shown  you  in  this  video  a  couple  of 
 hours  ago  as  how  to  get  it  installed 
 locally  the  1.3  billion  model  and  then 
 produce  videos  from  text  with  very  very 
 good  quality  in  this  video  though  I'm 
 going  to  show  it  to  you  as  how  you  can 
 install  14  billion  parameter  model  model 
 locally  and  then  we  will  also  generate 
 videos  from  images  so  this  model  comes 
 in  two  variants  1.3  billion  parameter 
 variant  and  14  billion  one  and  I  will 
 also  shortly  explain  the  difference  in 
 bit  more  detail 
 so  for  both  models  um  you  can  go  with 
 text  to  video  but  for  image  to  video  you 
 would  need  to  use  this  14  billion 
 parameter  model  and  that  is  what  we  are 
 going  to  do  in  this  video  after  getting 
 it  installed  loly  but  before  that  let's 
 check  out  this  model  bit  more  in  detail 
 especially  from  the  architectural  point 
 of  view  because  I  believe  this  model  is 
 going  to  shape  the  way  we  think  about 
 this  AI  video  generation  with  text  so  in 
 front  of  you  this  is  something  called  as 
 variational  Auto  encoder  it's  a  new  3D 
 causal  VA  or  V  architecture  as  we  call 
 it  which  has  been  specifically  designed 
 for  video  generation  so  what  they  have 
 done  here  is  they  have  combined  multiple 
 strategies  to  improve  spatio  temporal 
 compression  reduce  memory  usage  and 
 ensure  temporal  causality  these 
 enhancements  not  only  make  their  we  more 
 efficient  and  scalable  but  also  better 
 suited  for  integration  with  diffusion 
 based  generative  Motors  like  diffusion 
 Transformer  and  that  is  evident  from  a 
 lot  of  videos  which  they  have  shared  and 
 I  have  to  reload  this  page  I'm  sorry 
 because  this  is  such  a  heavy  page  but 
 still  you  can  see  that  the  quality  of 
 videos  is  Simply  Sublime  look  at  this 
 one  I'll  just  wait  for  it  to  load  and 
 run  and  then  look  at  this 
 one  so  this  is  what  I  was  talking  about 
 this  is  coming  from  V  look  at 
 this  how  realistic  this  one  looks 
 now  let's  try  to  get  it  installed  and 
 while  it  gets  installed  I  will  be 
 explaining  it  more  so  that  we  don't 
 waste  time  before  that  let  me  also  give 
 a  huge  shout  out  to  Mast  compute  who  are 
 sponsoring  the  VM  and  GPU  for  this  video 
 this  is  the  VM  I'm  going  to  use  12 
 22.041  a6000  with  48  GP  of  vram  so  let's 
 first  create  Koda  environment  and  while 
 that's  get  created  let's  check  out  bit 
 more  videos  look  at  this  one  hopefully 
 it  is  going  to  show  it  to  me  it  is  just 
 loading  so  let's 
 wait  okay  meanwhile  we  will  have  a  look 
 here  so  it  is  controllable  editing  look 
 at 
 this  wow  so  you  can  even  edit  it  quite 
 nicely  but  I'm  not  going  to  show  you 
 editing  in  this  video  I  will  create 
 another  one  because  that  requires  a 
 full-blown  video  okay  let's  check  out 
 this  one  wow  so  you  can  even  do  the 
 visual  text  generation  and  that  is  one 
 thing  which  was  missing  from  the  sorry 
 from  the  previous 
 models  okay  let's  go  back  meanwhile  to 
 see  what  is  happening  our  environment  is 
 created  let  me  also  create  a  k 
 environment  here  uh  sorry  um  Cuda 
 installation  because  I  have  an  VDS  GPU 
 this  is  going  to  take  a 
 minute  so  you  see  how  good  that  looks 
 especially  look  at  this  um  text  on  this 
 video 
 and  look  at  this  MAD  racing  on  the  right 
 hand 
 side  pretty  cool  isn't  it  they  also  have 
 shared  the  prompt  maybe  I  will  also  try 
 out  this 
 prompt  but  this  is  a  text  one  so  we  will 
 check  out  the  image  to  video  one  look  at 
 this  this  is  a  sound  effect  I  have 
 turned  it  off  I'm  not  sure  if  YouTube 
 would  allow  me  but  you  can  switch  it  on 
 because  I  will  drop  the  link  to  it  in 
 video's 
 description  look  at  the 
 conert  okay  while  that  happens  let  me 
 also  introduce  you  to  the  sponsors  of 
 the  video  who  are  ENT  bot  ENT  bot  lets 
 you  effortlessly  deploy  a  personalized 
 knowledge  bot  across  platforms  like 
 Discord  slack  and  others  it  is  ideal  for 
 open  source  Tech  communities  and 
 startups  that  provide  user  support  and  I 
 will  drop  the  link  to  their  website  in 
 videos 
 description  okay  let's  go  check  our 
 terminal  and  these  prerequisites  are 
 done  let  me  clear  the  screen  and  we  will 
 get  clone  the 
 repo  and  I'm  cloning  the  repo  of  it  and 
 seeding  into  it  shouldn't  take  too  long 
 that  is  done  let's  install  all  the 
 requirements  from  the  root  of  the  repo 
 primarily  it  installs  the  torch  torch 
 vision  and  takes  around  couple  of 
 minutes  and  while  that  happens  let's 
 check  out  bit  more  videos  here  look  at 
 this  product  feature  again  this  is  a 
 text  to  video  one  and  look  at  the 
 Quality  this  is  really  quite  inspiring  I 
 would  so  wow  let  me  look  at  the  eil 
 tower  it  look  quite 
 good  okay  let's  go  here  check  out  this 
 one  again  Sublime  quality  look  at  this 
 one  focus  on  the  hands  here  they  are 
 quite  well 
 formed  and  then  all  the  samples  look 
 pretty  neat  to 
 me  look  at  the  smoke  coming  out  of 
 it  okay  so  I'm  just  going  to  close  it 
 and  let's  have  a  quick  look  on  some  of 
 the  other  things  which  they  have  been  um 
 mentioning  here  in  this  tech  report  so 
 if  you  look  here  primarily  what  this  is 
 showing  you  is  that  the  area  of  sphere 
 represents  the  size  of  model  parameter 
 and  the  default  compression  rate  is  4 
 into  8  into  8  so  this  is  showing  that 
 that  you  know  um  the  performance  is 
 quite  competitive  uh  across  both 
 metrics  whether  it  uh  you  know  it  is 
 convolution  module  or  it  is  any 
 compression  ratio  so  the  number  of 
 frames  in  each  processing  chunk  is 
 limited  to  at  most  four  by  the  way  and 
 that  is  preventing  GPU  memory  overflow 
 so  state-ofthe-art  results  here  in  this 
 one  and  then  similarly  these  are  some  of 
 the  other  stuff  which  they  have  shared 
 here  for  example  this  is  around  their 
 video  diffusion  Transformer  uh  because 
 this  van  2.1  is  designed  using  the  flow 
 matching  framework  within  the  Paradigm 
 of  mainstream  Fusion  Transformer  and 
 they  are  also  utilizing  T5  encoder  to 
 encode  the  input  multilanguage  text  that 
 incorporates  cross  attention  within  each 
 Transformer  block  to  embed  the  text  into 
 model  structure  they  have  also  employed 
 a  linear  layer  and  cayer  to  process  the 
 input  time  imp  addings  and  then  this  is 
 also  shown  here  and  here  where  there  are 
 showing  some  of  the  theoretical  versus 
 actual  speed  up  and  this  is  again  the 
 thing  about  the  clip  which  I  was 
 discussing  and  by  the  way  this  MLP  is 
 shared  across  all  their  Transformer 
 blogs  within  and  with  each  block 
 learning  a  distinct  set  of  biases  and 
 that  really  really  lifts  up  the  whole  uh 
 quality  of  this  model  and  then  similarly 
 they  have  shared  some  of  the  other  data 
 around  the  training  and  all  that  stuff 
 so  for  example  this  is  mainly  around 
 their  training  data  set  as  they  have 
 curated  and  duplicated  a  candidate  data 
 set  compress  ing  1.5  billion  videos  and 
 10  billion  images  sourced  from  both  of 
 their  internal  copyrighted  sources  and 
 publicly  accessible  data  in  the 
 pre-training  phase  their  goal  was  to 
 select  high  quality  and  diverse  data 
 from  this  expensive  yet  noisy  data  set 
 to  facilitate  effective  training  and 
 throughout  the  data  mining  process  they 
 have  designed  a  four-step  data  cleaning 
 process  which  you  can  see  on  the  screen 
 focusing  on  fundamental  fundamental 
 Dimensions  visual  quality 
 and  motion  quality  okay  so  I  hope  that 
 now  you  understand  bit  more  around  its 
 training  and  its  architecture  let's  go 
 to  our  terminal  and  see  what  is 
 happening 
 there  okay  all  the  prerequisites  are 
 done  which  is  great  now  let  me  install 
 um  log  to  the  hugging  phas  you  would 
 need  your  hugging  phas  free  read  token 
 which  you  can  obtain  from  your  hugging 
 face.  profile  so  make  sure  that  you  have 
 that  and  we  needed  to  download  the  model 
 from  hugging  pH  so  so  I'm  just  putting 
 in  my  token  here  and  then  my  login  is 
 successful  next  up  let's  go  to  their 
 gradio  folder  and  from  there  we  need  to 
 run  our  image  to  video  and  if  you  want 
 to  run  the  text  to  video  if  you  just  LS 
 here  you  will  see  that  they  have  shared 
 scripts  for  all  of  it  so  I'm  just 
 because  I  have  single  GPU  so  I'm  just 
 going  to  run  this  image  to  video  14 
 billion  on  single  GPU  first  it  is  going 
 to  download  the  model  and  then  it  is 
 going  to  launch  our  gradio  demo 
 hopefully  uh  I'll  be  honest  I  haven't 
 already  tested  it  yet  okay  so  it  says 
 that  it  couldn't  find  the  checkpoint  so 
 what  it  means  is  that  I  would  need  to 
 specify  the 
 checkpoint  um  or  I  would  first  need  to 
 download  it  that  is  not  a  problem  let  me 
 quickly  download 
 that  because  we  are  already  logged  in  so 
 I'll  just  go  to  hugging  face  page  and 
 grab  that  model 
 so  now  as  we  already  are  logged  into 
 hugging  phas  I'll  just  go  to  the  root  of 
 the  repo  like  this  let  me  clear  the 
 screen  and  then  in  order  to  download  the 
 model  I'm  just  going  to  use  this  huging 
 F  CLI  download  and  it  is  going  to 
 download  all  the 
 models  and  there  are  it  seems  seven 
 shards  of  it  so  let's  wait  for  it  to  get 
 downloaded  and  then  we  will  proceed 
 further  and  the  model  download  is  almost 
 there 
 and  all  the  models  have  been  downloaded 
 let  me  quickly  show  you  the  space  on 
 disk  there  occupying  so  around  77  gig  as 
 you  can  see  on  the  left  maybe  let  me 
 clear  the  screen  and  show  you  there  you 
 go  77  gig  so  make  sure  that  you  have 
 that  much 
 space  okay  so  now  let's  run  our  gradio 
 demo  and  for  that  I'm  just  going  to  run 
 this  command  so  again  I'm  calling  that 
 single  GPU  uh  IM  to  video  with  14 
 billion  and  I  am  specifying  this 
 parameter  with  my  checkpoint  directory 
 which  is  again  the  one  where  we  have 
 just  downloaded  the  model  so  let  me  run 
 it  and  it  is  running  and  it  is  also 
 downloading  one  remaining  model  I 
 believe  so  let's  wait  for 
 it  and  the  model  is  loaded  and  running 
 on  our  local  host  at  Port 
 7860  let  me  access 
 it  and  there  you  go  so  our  when  2.1  one 
 is  running  the  image  to  video  with  14 
 billion  parameter  and  you  can  select  the 
 resolution  from  here  whether  it  is  720 
 pixel  or  480  I'll  just  go  with  the  721 
 if  it  allows  me  to 
 select  okay  we'll  try  again  maybe  I'll 
 just  do  the  480  what  it  does 
 here  okay  it  is  processing  something 
 anyway  I  will  let  it  process  whatever  it 
 is  doing  maybe  default  is  720  and  then 
 you  can  upload  your  image  here  you  can 
 Define  your  prompt  I'll  go  with  English 
 and  then  you  can  also  do  some  prompt 
 extension  or  enhancement  with  um  some  of 
 the  shift  scale  and  seed  and  all  that 
 stuff  I  already  have  explained  these 
 parameters  in  this  first  video  and  then 
 you  can  generate  the  video  here  okay  so 
 let's  try  it 
 out  so  I  have  selected  this  image  of  a 
 bear  and  then  I  have  given  it  this 
 prompt  I  also  enhanced  it  by  the  way  by 
 just  clicking  on  prompt  enhance  that  I 
 me  stick  Brown  beard  stands  in  a  shallow 
 water  it's  for  glistening  with  moisture 
 and  all  that  stuff  so  it  has  done  a 
 pretty  well  vision  and  then  I'm  not  just 
 going  to  put  it  any  negative  prompt  here 
 I'm  just  going  to  click  on  generate 
 video  and  then  let's  quickly  check  the 
 vram  consumption  too  I'll  do  it  again  so 
 it  is  consuming  close  to  17  gig  of  vram 
 for  this  image2  video  model  let's  go  up 
 okay  let's  wait  for  it  to  come  back 
 it  is  taking  bit  of  a  time  to  generate 
 the  video  so  let's  see  how  long  does  it 
 take  and  let's  also  check  the  vram 
 consumption  again  so  it  is  consuming 
 around  14  gig  of  vram  in  total  it  seems 
 I'll  just  check  it  again  to  make  sure 
 now  it  is  uh  going  up  I  think  this  is 
 the  max  I  have  seen  so  far  just  under  19 
 gig  nopes  this  time  it  has  gone  up 
 that's  good  that  we  are  so  let's  monitor 
 it  in  real 
 time  it  is  still  going  up  the  full  model 
 the  14  billion  one  it  seems  it  is 
 loading  so  I'll  just  wait  for  it  a  bit 
 more  and  then  I'll  keep 
 checking  okay  it  is  climbing  up 
 hopefully  it  will  fit  onto  my  GPU  and 
 doesn't  time  out  cuz  my  GPU  is  48  GB  it 
 has  already  reached  to 
 36  41  this  is  just  like  a  some  mat 
 happening  let's 
 see  46  now  maybe  close  to  getting  timed 
 out  and  it  has 
 already  almost  on  the  Heil  of  my  GPU 
 vram  it  is  not  going  up  it  is  consuming 
 48.5  GP  of  vram  and  it  is  still  running 
 so  it  is  because  there  is  no  more  room 
 there  I'll  just  close  it  let's  see  here 
 and  you  can  see  that  it  has  gone  out  of 
 memory  so  in  this  case  what  I'm  going  to 
 do  I'm  just  going  to  switch  over  to  um 
 another  GPU  and  I  will  show  you  what 
 happens 
 there  okay  so  I  have  moved  to  the  new 
 GPU  which  is  A1  180gb  and  I  will  also 
 show  you  the  vram  consumption  in  a  bit  I 
 actually  thought  of  just  making  a  new 
 video  instead  of  carrying  it  on  but  I 
 thought  okay  maybe  instead  of  editing  it 
 and  showing  you  a  very  clean  crisp 
 version  I'll  show  you 
 my  day-to-day  life  struggles  with  these 
 sort  of  stuff  that  it  is  not  that  in 
 just  one  go  you  get  these  things  right 
 so  if  you're  struggling  don't  worry 
 everyone  goes  through  that  so  you  see 
 what  I  have  done  here  is  again  I  have 
 selected  the  image  prompt  has  been 
 enhanced  and  then  I  have  just  clicked  on 
 generate  video  and  it  is  currently 
 generating  the  video  now  let's  check  out 
 the  VM  consumption  so  you  see  um  my  vram 
 is  80GB  and  it  is  totally  consuming  all 
 of  it  at  the  moment 
 80GB 
 now  if  I  take  you  to  my  terminal  you  see 
 it  is  generating  the  video  and  even  uh 
 it  is  on  totally  on  my  80GB  V  Ram  it  is 
 still  taking  just  under  an  hour  so  57 
 minutes  to  generate  that  video  which  uh 
 we  are  generating  at  the  moment  this  is 
 a  majestic  brown  bear  and  all  that  stuff 
 if  we  already  saw  there  so  let's  see  I 
 will  wait  for  it  to  generate  the  video 
 and  then  we  will  check  it  out  I  will 
 wait  for  1 
 hour  and  at  last  it's  done  let's  check 
 out  the  video  it  should  be  in  the  root 
 of  the 
 repo  and  there  you  go  so  the  mage2  video 
 is  ready  let's  play  it  in  the  video 
 player  there  you  go  not  bad  so  beer  is 
 dancing  in  the  water  that  is  what  we 
 asked  it  to  do  let  me  play  again 
 so  beer  looks  good  and  it's  the  same 
 beer  which  was  uh  Brown  beer  and  the 
 water  is  bit 
 changed  but  other  than  that  I  think  not 
 bad  at  all  the  beer  limbs  are  also  okay 
 and  even  the  water  is  tripping  look  at 
 the  water  motion  so  the  physics  is  right 
 too  pretty  nice  very  very  nice  and  I'm 
 not  sure  if  we  can  increase  the  length 
 of  the  video  I  know  that  you  guys  are 
 going  to  ask  it  but  you  already  saw  that 
 on  even  for  this  one  14  billion 
 parameter  one  uh  it  was  consuming  80  gb 
 and  took  an  hour  to  get  it  generated  and 
 just  in  case  if  you're  wondering  what 
 happened  to  the  gradio  one  so  while  it 
 was  running  it  started  giving  me  this 
 error  so  it  crashed  on  Me  Maybe  due  to 
 the  vrm  so  I  couldn't  play  the  video  in 
 the  gradio  and  but  it  was  there  in  the 
 root  of  the  repo  so  I  hope  that  this  was 
 helpful  let  me  know  how  you  go  and  if 
 again  if  you  don't  have  that  much  vrm 
 don't  worry  just  watch  this  video  um  on 
 your  screen  so  this  is  where  I  have 
 shown  you  how  to  do  the  1.3  billion  one 
 you  can't  do  the  image  to  video  with 
 that  but  text  to  video  works  fine  check 
 it  out  let  me  know  how  you  go  if  you 
 like  the  content  please  consider 
 subscribing  to  the  channel  and  if  you're 
 already  subscribed  please  share  it  among 
 your  network  as  that  is  the  only 
 marketing  I  do  thank  you  very  much